# INE5454 - Ger√™ncia de Dados
## Web Scraper para Coleta de Dados de PS5

Este projeto implementa um web scraper para coletar dados de produtos PS5 de diversos sites de com√©rcio eletr√¥nico brasileiros.

### Sites Monitorados
- MercadoLivre (https://lista.mercadolivre.com.br/ps5)
- Kabum (https://www.kabum.com.br/gamer/playstation/consoles-playstation/playstation-5)
- Magazine Luiza (https://www.magazineluiza.com.br/busca/ps5/)
- Amazon Brasil (https://www.amazon.com.br/s?k=ps5)
- Casas Bahia (https://www.casasbahia.com.br/ps5/b)

### Dados Coletados
Cada produto coletado cont√©m os seguintes atributos:
- **preco_vista**: Pre√ßo √† vista do produto
- **preco_parcelado**: Pre√ßo parcelado (quando dispon√≠vel)
- **modelo**: Modelo do PS5 (Standard, Digital Edition, Slim)
- **nome_anuncio**: Nome/t√≠tulo do an√∫ncio
- **link_pagina**: URL da p√°gina do produto
- **tipo**: Tipo do produto (Console)
- **cor**: Cor do produto
- **com_leitor_disco**: Se possui leitor de disco (Sim/N√£o)
- **espaco_armazenamento**: Capacidade de armazenamento
- **jogos_incluidos**: Jogos inclu√≠dos no pacote
- **inclui_controles**: Se inclui controles (Sim/N√£o)
- **marca**: Marca do produto (Sony)

### Instala√ß√£o e Execu√ß√£o

#### 1. Teste do Ambiente
```bash
# Verifica se todas as depend√™ncias est√£o funcionando
python3 test_environment.py
```

#### 2. Instala√ß√£o das Depend√™ncias
```bash
# Instalar depend√™ncias Python
pip install -r requirements.txt

# Instalar Chrome e ChromeDriver (Ubuntu/Debian)
sudo apt update
sudo apt install google-chrome-stable
wget -O /tmp/chromedriver.zip "https://storage.googleapis.com/chrome-for-testing-public/140.0.7339.207/linux64/chromedriver-linux64.zip"
sudo unzip /tmp/chromedriver.zip -d /usr/local/bin/
sudo chmod +x /usr/local/bin/chromedriver
sudo ln -sf /usr/local/bin/chromedriver /usr/bin/chromedriver
```

#### 3. Execu√ß√£o do Scraper
```bash
# Executar scraper completo (usa padr√£o de 50 p√°ginas por site)
python3 ps5_scraper.py

# Executar com limite personalizado de p√°ginas por site
export MAX_PAGES_PER_SITE=10
python3 ps5_scraper.py

# Ou definir inline
MAX_PAGES_PER_SITE=5 python3 ps5_scraper.py
```

#### 4. Configura√ß√£o de Vari√°veis de Ambiente

O scraper suporta as seguintes vari√°veis de ambiente:

- **MAX_PAGES_PER_SITE**: Define o n√∫mero m√°ximo de p√°ginas a coletar por site (padr√£o: 50)
  ```bash
  export MAX_PAGES_PER_SITE=10  # Coleta no m√°ximo 10 p√°ginas por site
  python3 ps5_scraper.py
  ```

### Arquivos do Projeto

#### Scripts Principais
- `ps5_scraper.py` - Scraper principal com todas as funcionalidades
- `test_environment.py` - Teste do ambiente e depend√™ncias
- `demo.py` - Demonstra√ß√£o com um site

#### Arquivos de Configura√ß√£o
- `requirements.txt` - Depend√™ncias Python
- `config.py` - Configura√ß√µes personaliz√°veis
- `install_and_run.sh` - Script de instala√ß√£o autom√°tica

#### Sa√≠das Geradas
- `ps5_products.json` - Dados em formato JSON
- `ps5_products.xlsx` - Dados em formato Excel
- `scraper.log` - Log detalhado da execu√ß√£o

### Funcionalidades

- ‚úÖ Coleta configur√°vel de p√°ginas por site (via vari√°vel de ambiente MAX_PAGES_PER_SITE, padr√£o: 50)
- ‚úÖ Remo√ß√£o de duplicatas
- ‚úÖ Tratamento robusto de erros
- ‚úÖ Delays aleat√≥rios para evitar bloqueios
- ‚úÖ Logging detalhado com emojis
- ‚úÖ Sa√≠da em JSON e Excel
- ‚úÖ Estat√≠sticas detalhadas
- ‚úÖ Configura√ß√µes personaliz√°veis

### Depend√™ncias
- Python 3.7+
- requests
- beautifulsoup4
- selenium
- lxml
- fake-useragent
- pandas
- openpyxl
- Google Chrome + ChromeDriver

### Solu√ß√£o de Problemas

#### Chrome n√£o encontrado
```bash
sudo apt update
sudo apt install google-chrome-stable
```

#### ChromeDriver n√£o encontrado
```bash
wget -O /tmp/chromedriver.zip "https://storage.googleapis.com/chrome-for-testing-public/140.0.7339.207/linux64/chromedriver-linux64.zip"
sudo unzip /tmp/chromedriver.zip -d /usr/local/bin/
sudo chmod +x /usr/local/bin/chromedriver
```

#### Depend√™ncias Python
```bash
pip install -r requirements.txt
```

#### Teste do ambiente
```bash
python3 test_environment.py
```

### Exemplo de Uso

```bash
# 1. Testar ambiente
python3 test_environment.py

# 2. Executar scraper
python3 ps5_scraper.py

# 3. Verificar resultados
ls -la *.json *.xlsx *.log
```

### Resultados Esperados

O scraper ir√° gerar:
- **ps5_products.json**: Dados estruturados em JSON
- **ps5_products.xlsx**: Planilha Excel para an√°lise
- **scraper.log**: Log detalhado da execu√ß√£o
- **Estat√≠sticas**: Resumo dos dados coletados

### Observa√ß√µes
- O scraper inclui delays aleat√≥rios entre requisi√ß√µes para evitar bloqueios
- Remove duplicatas baseado no link do produto
- Coleta dados de m√∫ltiplas p√°ginas de cada site
- Inclui tratamento de erros robusto e retry autom√°tico
- Gera logs detalhados para debugging
- Suporta sa√≠da em JSON e Excel
- Inclui estat√≠sticas detalhadas dos dados coletados

---

## Sistema de Busca de Video Games (Next.js)

Aplica√ß√£o web desenvolvida em Next.js 16.0.7 para buscar, filtrar e ordenar video games a partir dos dados coletados pelo scraper.

### Funcionalidades

- üîç **Busca em tempo real** - Busca por texto em todos os campos do produto
- üéØ **Filtros avan√ßados**:
  - Range de pre√ßo (m√≠nimo e m√°ximo)
  - Modelo (select m√∫ltiplo)
  - Tipo (select m√∫ltiplo)
  - Marca (select m√∫ltiplo)
  - Site de origem (checkboxes)
  - Inclui controles (Sim/N√£o)
  - Inclui jogos (checkbox)
  - Espa√ßo em disco (range em GB)
- üìä **Ordena√ß√£o** - Por pre√ßo (menor para maior / maior para menor)
- üì± **Design responsivo** - Interface otimizada para mobile e desktop
- üìÑ **Pagina√ß√£o** - Navega√ß√£o entre p√°ginas de resultados

### Instala√ß√£o e Execu√ß√£o

#### 1. Instalar depend√™ncias
```bash
npm install
# ou
yarn install
# ou
pnpm install
```

#### 2. Executar em modo desenvolvimento
```bash
npm run dev
# ou
yarn dev
# ou
pnpm dev
```

A aplica√ß√£o estar√° dispon√≠vel em `http://localhost:3000`

#### 3. Build para produ√ß√£o
```bash
npm run build
npm start
```

### Estrutura do Projeto

```
‚îú‚îÄ‚îÄ app/
‚îÇ   ‚îú‚îÄ‚îÄ api/
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ products/
‚îÇ   ‚îÇ       ‚îî‚îÄ‚îÄ route.ts          # API endpoint de busca/filtro
‚îÇ   ‚îú‚îÄ‚îÄ globals.css                # Estilos globais (Tailwind)
‚îÇ   ‚îú‚îÄ‚îÄ layout.tsx                 # Layout principal
‚îÇ   ‚îî‚îÄ‚îÄ page.tsx                   # P√°gina principal
‚îú‚îÄ‚îÄ components/
‚îÇ   ‚îú‚îÄ‚îÄ FilterPanel.tsx            # Painel de filtros
‚îÇ   ‚îú‚îÄ‚îÄ ProductCard.tsx            # Card de produto
‚îÇ   ‚îú‚îÄ‚îÄ ProductList.tsx            # Lista de produtos
‚îÇ   ‚îî‚îÄ‚îÄ SearchBar.tsx              # Barra de busca
‚îú‚îÄ‚îÄ lib/
‚îÇ   ‚îú‚îÄ‚îÄ data-loader.ts             # Carregamento e processamento dos JSONs
‚îÇ   ‚îú‚îÄ‚îÄ types.ts                   # Tipos TypeScript
‚îÇ   ‚îî‚îÄ‚îÄ utils.ts                   # Fun√ß√µes utilit√°rias
‚îú‚îÄ‚îÄ magazineluiza_products.json    # Dados do Magazine Luiza
‚îî‚îÄ‚îÄ mercadolivre_products.json     # Dados do Mercado Livre
```

### Tecnologias Utilizadas

- **Next.js 16.0.7** - Framework React com App Router
- **TypeScript** - Tipagem est√°tica
- **Tailwind CSS** - Estiliza√ß√£o
- **React 18** - Biblioteca UI

### API Endpoints

#### GET `/api/products`

Busca e filtra produtos com os seguintes par√¢metros de query:

- `query` - Texto de busca
- `precoMin` - Pre√ßo m√≠nimo
- `precoMax` - Pre√ßo m√°ximo
- `modelo` - Modelos (separados por v√≠rgula)
- `tipo` - Tipos (separados por v√≠rgula)
- `marca` - Marcas (separados por v√≠rgula)
- `site_origem` - Sites (separados por v√≠rgula)
- `inclui_controles` - "Sim" ou "N√£o"
- `inclui_jogos` - "true" ou "false"
- `espacoMin` - Espa√ßo m√≠nimo em GB
- `espacoMax` - Espa√ßo m√°ximo em GB
- `sortBy` - "preco_asc" ou "preco_desc"
- `page` - N√∫mero da p√°gina (padr√£o: 1)
- `limit` - Itens por p√°gina (padr√£o: 20)

**Exemplo de resposta:**
```json
{
  "products": [...],
  "total": 150,
  "page": 1,
  "limit": 20,
  "totalPages": 8,
  "filters": {
    "modelos": [...],
    "tipos": [...],
    "marcas": [...],
    "sites": [...],
    "precoMin": 0,
    "precoMax": 10000,
    "espacoMin": 0,
    "espacoMax": 2000
  }
}
```

### Notas T√©cnicas

- Os dados s√£o carregados dos arquivos JSON na inicializa√ß√£o e mantidos em cache
- Pre√ßos s√£o normalizados para lidar com diferentes formatos ("3999.00" vs "4.499")
- Espa√ßo em disco √© extra√≠do do texto livre usando regex e convertido para GB
- A busca √© case-insensitive e busca em m√∫ltiplos campos
- Interface mobile-friendly com filtros em drawer no mobile